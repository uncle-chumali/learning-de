{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading CitiBike Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_csv = glob.glob(r\"data/JC-[0-9][0-9][0-9][0-9][0-9][0-9]-citibike-tripdata.csv\")\n",
    "bike_data = []\n",
    "\n",
    "for file in bike_csv:\n",
    "    df_bike = pd.read_csv(file)\n",
    "    bike_data.append(df_bike)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all the CSV's into a single DataFrame to be analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bike = pd.concat(bike_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Checking Column Names and Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print column names to determine whether processing necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Trip Duration', 'Start Time', 'Stop Time', 'Start Station ID',\n",
      "       'Start Station Name', 'Start Station Latitude',\n",
      "       'Start Station Longitude', 'End Station ID', 'End Station Name',\n",
      "       'End Station Latitude', 'End Station Longitude', 'Bike ID', 'User Type',\n",
      "       'Birth Year', 'Gender'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_bike.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The white space, and capitalised characters complicate preparation of data. In order to simplify data preparation the columns are then renamed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename all the columns to remove capitalised characters and replace whitespace with underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bike.columns = [x.replace(' ', '_').lower() for x in df_bike.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the `bike_id` the index so that it can become the primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bike['bike_id'] = df_bike.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the columns to verify that the renaming of the columns was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['trip_duration', 'start_time', 'stop_time', 'start_station_id',\n",
      "       'start_station_name', 'start_station_latitude',\n",
      "       'start_station_longitude', 'end_station_id', 'end_station_name',\n",
      "       'end_station_latitude', 'end_station_longitude', 'bike_id', 'user_type',\n",
      "       'birth_year', 'gender'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_bike.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the datatypes assigned to each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_duration                int64\n",
      "start_time                  object\n",
      "stop_time                   object\n",
      "start_station_id             int64\n",
      "start_station_name          object\n",
      "start_station_latitude     float64\n",
      "start_station_longitude    float64\n",
      "end_station_id               int64\n",
      "end_station_name            object\n",
      "end_station_latitude       float64\n",
      "end_station_longitude      float64\n",
      "bike_id                      int64\n",
      "user_type                   object\n",
      "birth_year                 float64\n",
      "gender                       int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_bike.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the column data types to their appropriate values. Trip duration is changed to be `stop_time - start_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bike['start_time'] = df_bike.start_time.astype('datetime64[s]')\n",
    "df_bike['stop_time'] = df_bike.stop_time.astype('datetime64[s]')\n",
    "df_bike['trip_duration'] = df_bike.apply(lambda row: (row['stop_time'] - row['start_time']).total_seconds(), axis=1)\n",
    "df_bike['user_type'] = pd.Categorical(df_bike.user_type, ['Subscriber', 'Customer'])\n",
    "df_bike['start_station_name'] = df_bike.start_station_name.astype('string')\n",
    "df_bike['end_station_name'] = df_bike.end_station_name.astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We cannot cast the `birth_year` into an integer value as it contains missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then validate that the datatypes have been defined correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_duration                     float64\n",
      "start_time                  datetime64[s]\n",
      "stop_time                   datetime64[s]\n",
      "start_station_id                    int64\n",
      "start_station_name         string[python]\n",
      "start_station_latitude            float64\n",
      "start_station_longitude           float64\n",
      "end_station_id                      int64\n",
      "end_station_name           string[python]\n",
      "end_station_latitude              float64\n",
      "end_station_longitude             float64\n",
      "bike_id                             int64\n",
      "user_type                        category\n",
      "birth_year                        float64\n",
      "gender                              int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_bike.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to describe the data to see if the summary statistics indicate the data is as we would expect it to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>birth_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.475840e+05</td>\n",
       "      <td>247584.000000</td>\n",
       "      <td>247584.000000</td>\n",
       "      <td>247584.000000</td>\n",
       "      <td>247584.000000</td>\n",
       "      <td>228585.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.861297e+02</td>\n",
       "      <td>40.723121</td>\n",
       "      <td>-74.046438</td>\n",
       "      <td>40.722594</td>\n",
       "      <td>-74.045855</td>\n",
       "      <td>1979.335276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.593810e+04</td>\n",
       "      <td>0.008199</td>\n",
       "      <td>0.011211</td>\n",
       "      <td>0.007958</td>\n",
       "      <td>0.011283</td>\n",
       "      <td>9.596809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.271000e+03</td>\n",
       "      <td>40.692640</td>\n",
       "      <td>-74.096937</td>\n",
       "      <td>40.692216</td>\n",
       "      <td>-74.096937</td>\n",
       "      <td>1900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.490000e+02</td>\n",
       "      <td>40.717732</td>\n",
       "      <td>-74.050656</td>\n",
       "      <td>40.716540</td>\n",
       "      <td>-74.050444</td>\n",
       "      <td>1974.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.900000e+02</td>\n",
       "      <td>40.721525</td>\n",
       "      <td>-74.044247</td>\n",
       "      <td>40.721124</td>\n",
       "      <td>-74.043117</td>\n",
       "      <td>1981.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.670000e+02</td>\n",
       "      <td>40.727596</td>\n",
       "      <td>-74.038051</td>\n",
       "      <td>40.727224</td>\n",
       "      <td>-74.036486</td>\n",
       "      <td>1986.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.632981e+07</td>\n",
       "      <td>40.752559</td>\n",
       "      <td>-74.032108</td>\n",
       "      <td>40.801343</td>\n",
       "      <td>-73.957390</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       trip_duration  start_station_latitude  start_station_longitude  \\\n",
       "count   2.475840e+05           247584.000000            247584.000000   \n",
       "mean    8.861297e+02               40.723121               -74.046438   \n",
       "std     3.593810e+04                0.008199                 0.011211   \n",
       "min    -3.271000e+03               40.692640               -74.096937   \n",
       "25%     2.490000e+02               40.717732               -74.050656   \n",
       "50%     3.900000e+02               40.721525               -74.044247   \n",
       "75%     6.670000e+02               40.727596               -74.038051   \n",
       "max     1.632981e+07               40.752559               -74.032108   \n",
       "\n",
       "       end_station_latitude  end_station_longitude     birth_year  \n",
       "count         247584.000000          247584.000000  228585.000000  \n",
       "mean              40.722594             -74.045855    1979.335276  \n",
       "std                0.007958               0.011283       9.596809  \n",
       "min               40.692216             -74.096937    1900.000000  \n",
       "25%               40.716540             -74.050444    1974.000000  \n",
       "50%               40.721124             -74.043117    1981.000000  \n",
       "75%               40.727224             -74.036486    1986.000000  \n",
       "max               40.801343             -73.957390    2000.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bike[['trip_duration', 'start_station_latitude', 'start_station_longitude', 'end_station_latitude', 'end_station_longitude', 'birth_year']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dictionary states that if a value of zero is given for gender then the gender is unknown. We can therefore convert zero values into `pd.nan` values so that we can evaluate how much data is missing with `df_bike.isnull().sum()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a *lambda* function, we convert any value of gender that is a `0` value to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bike['gender'] = df_bike.gender.apply(lambda x: None if x != 1 and x != 2 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then observe how much of the data is missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_duration                  0\n",
      "start_time                     0\n",
      "stop_time                      0\n",
      "start_station_id               0\n",
      "start_station_name             0\n",
      "start_station_latitude         0\n",
      "start_station_longitude        0\n",
      "end_station_id                 0\n",
      "end_station_name               0\n",
      "end_station_latitude           0\n",
      "end_station_longitude          0\n",
      "bike_id                        0\n",
      "user_type                    380\n",
      "birth_year                 18999\n",
      "gender                     19901\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_bike.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "1. The `trip_duration` max value is an order of magnitude above data in the 3 percentiles.\n",
    "1. The min `birth_year` is conveniently `1900`.\n",
    "1. The `birth_year` has missing data.\n",
    "1. The `user_type` has missing data.\n",
    "1. The data dictionary states that the `gender` is unknown when the value is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Cleaning CitiBike Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above observations, we can begin cleaning the data so that it is ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trip Duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to view the outliers, we should inspect the data for the `trip_duration` column in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3271. -2484.    61.    61.    61.    61.    61.    61.    61.    61.]\n",
      "[ 1120971.  1258737.  1532001.  1569766.  1837255.  2071209.  2100552.\n",
      "  2104124.  4826890. 16329808.]\n"
     ]
    }
   ],
   "source": [
    "print(np.sort(df_bike.trip_duration)[:10])\n",
    "print(np.sort(df_bike.trip_duration)[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trip duration cannot have negative value, meaning that these values are incorrect and should be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing Negative Trip Durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bike = df_bike[df_bike.trip_duration > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Invalid Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CitiBike state that the longest valid trip duration is 24 hours. Therefore any data that lasts longer than this amount of time is considered invalid. This data should not be removed as there may be a reason for it (such as faulty bikes or improper use) and may be of interest to analysts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new column will be introduced named `valid_trip_duration`. This will have a value of `1` if the trip was under `86400` seconds (number of seconds in a day) or `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bike['valid_trip_duration'] = df_bike.trip_duration.apply(lambda t: 1 if t < 86400 else 0).astype('bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Birth Year Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to view our outliers, we should inspect the data for the `birth_year` column in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1900. 1934. 1937. 1937. 1937. 1937. 1940. 1940. 1940. 1941.]\n"
     ]
    }
   ],
   "source": [
    "print(np.sort(df_bike.birth_year.values)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is only a single outlier, hence we can remove the data at this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bike = df_bike[df_bike.birth_year != 1900]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Birth Year Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_birth_year = df_bike[df_bike[['birth_year']].isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     trip_duration          start_time           stop_time  start_station_id  \\\n",
      "125         3666.0 2016-02-01 09:56:46 2016-02-01 10:57:52              3212   \n",
      "148         1082.0 2016-02-01 11:43:51 2016-02-01 12:01:53              3183   \n",
      "154         1101.0 2016-02-01 12:04:35 2016-02-01 12:22:56              3192   \n",
      "163         1580.0 2016-02-01 12:26:09 2016-02-01 12:52:29              3192   \n",
      "168         1620.0 2016-02-01 12:53:44 2016-02-01 13:20:44              3186   \n",
      "174          558.0 2016-02-01 13:40:17 2016-02-01 13:49:35              3183   \n",
      "190          278.0 2016-02-01 14:51:43 2016-02-01 14:56:21              3187   \n",
      "202          756.0 2016-02-01 16:30:41 2016-02-01 16:43:17              3209   \n",
      "208         1020.0 2016-02-01 17:00:36 2016-02-01 17:17:36              3183   \n",
      "517         3412.0 2016-02-02 10:51:14 2016-02-02 11:48:06              3195   \n",
      "\n",
      "     start_station_name  start_station_latitude  start_station_longitude  \\\n",
      "125     Christ Hospital               40.734786               -74.050444   \n",
      "148      Exchange Place               40.716247               -74.033459   \n",
      "154  Liberty Light Rail               40.711242               -74.055701   \n",
      "163  Liberty Light Rail               40.711242               -74.055701   \n",
      "168       Grove St PATH               40.719586               -74.043117   \n",
      "174      Exchange Place               40.716247               -74.033459   \n",
      "190           Warren St               40.721124               -74.038051   \n",
      "202        Brunswick St               40.724176               -74.050656   \n",
      "208      Exchange Place               40.716247               -74.033459   \n",
      "517             Sip Ave               40.730743               -74.063784   \n",
      "\n",
      "     end_station_id    end_station_name  end_station_latitude  \\\n",
      "125            3185           City Hall             40.717732   \n",
      "148            3192  Liberty Light Rail             40.711242   \n",
      "154            3192  Liberty Light Rail             40.711242   \n",
      "163            3183      Exchange Place             40.716247   \n",
      "168            3203       Hamilton Park             40.727596   \n",
      "174            3202        Newport PATH             40.727224   \n",
      "190            3183      Exchange Place             40.716247   \n",
      "202            3183      Exchange Place             40.716247   \n",
      "208            3186       Grove St PATH             40.719586   \n",
      "517            3197            North St             40.752559   \n",
      "\n",
      "     end_station_longitude  bike_id user_type  birth_year  gender  \\\n",
      "125             -74.043845      125  Customer         NaN     NaN   \n",
      "148             -74.055701      148  Customer         NaN     NaN   \n",
      "154             -74.055701      154  Customer         NaN     NaN   \n",
      "163             -74.033459      163  Customer         NaN     NaN   \n",
      "168             -74.044247      168  Customer         NaN     NaN   \n",
      "174             -74.033759      174  Customer         NaN     NaN   \n",
      "190             -74.033459      190  Customer         NaN     NaN   \n",
      "202             -74.033459      202  Customer         NaN     NaN   \n",
      "208             -74.043117      208  Customer         NaN     NaN   \n",
      "517             -74.044725      517  Customer         NaN     NaN   \n",
      "\n",
      "     valid_trip_duration  \n",
      "125                 True  \n",
      "148                 True  \n",
      "154                 True  \n",
      "163                 True  \n",
      "168                 True  \n",
      "174                 True  \n",
      "190                 True  \n",
      "202                 True  \n",
      "208                 True  \n",
      "517                 True  \n"
     ]
    }
   ],
   "source": [
    "print(missing_birth_year.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From inspecting the head we can see that all records which are missing data from the `birth_year` appear to be of `user_type` `Customer`, and also of unknown `gender`. In order to validate that this pattern is consistent, we can calculate the proportion of missing birth years to total customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_type\n",
      "Subscriber     1.523222\n",
      "Customer      99.671413\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "proportion_customers_missing_birth_year = 100 * missing_birth_year.user_type.value_counts() / df_bike.user_type.value_counts()\n",
    "print(proportion_customers_missing_birth_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above analysis demonstrates that 99.7% of all our customer data do not include the data of birth. In order to investigate why, we should investigate methods of data collection.\n",
    "For now, we should not remove data where the birth year is missing as otherwise we would lose all our customer data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also saw that when the `birth_year` was missing, `gender` also appeared to be missing. To validate this we calculate the proportion of records with missing `gender` when `birth_year` is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "print(missing_birth_year.gender.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All records with a missing `birth_year` are also missing `gender`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Type Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We determined previously that if both the `gender` and the `birth_year` value are missing then the `user_type` is likely a `Customer`, else it is a `Subscriber`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_user_type = df_bike[df_bike.user_type.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how many records with a missing `user_type` could be a potential customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "potential_customer_count = missing_user_type[(missing_user_type.gender == 0) | (missing_user_type.birth_year.isnull())].shape[0]\n",
    "print(potential_customer_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that it is likely that all records that have a missing `user_type` are subscribers, however they may still be customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Resetting the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we dropped some data, lets reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>gender</th>\n",
       "      <th>valid_trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>247576</th>\n",
       "      <td>250.0</td>\n",
       "      <td>2016-05-31 23:16:00</td>\n",
       "      <td>2016-05-31 23:20:10</td>\n",
       "      <td>3186</td>\n",
       "      <td>Grove St PATH</td>\n",
       "      <td>40.719586</td>\n",
       "      <td>-74.043117</td>\n",
       "      <td>3209</td>\n",
       "      <td>Brunswick St</td>\n",
       "      <td>40.724176</td>\n",
       "      <td>-74.050656</td>\n",
       "      <td>247576</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247577</th>\n",
       "      <td>651.0</td>\n",
       "      <td>2016-05-31 23:18:32</td>\n",
       "      <td>2016-05-31 23:29:23</td>\n",
       "      <td>3209</td>\n",
       "      <td>Brunswick St</td>\n",
       "      <td>40.724176</td>\n",
       "      <td>-74.050656</td>\n",
       "      <td>3211</td>\n",
       "      <td>Newark Ave</td>\n",
       "      <td>40.721525</td>\n",
       "      <td>-74.046305</td>\n",
       "      <td>247577</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247578</th>\n",
       "      <td>2048.0</td>\n",
       "      <td>2016-05-31 23:25:28</td>\n",
       "      <td>2016-05-31 23:59:36</td>\n",
       "      <td>3199</td>\n",
       "      <td>Newport Pkwy</td>\n",
       "      <td>40.728745</td>\n",
       "      <td>-74.032108</td>\n",
       "      <td>3199</td>\n",
       "      <td>Newport Pkwy</td>\n",
       "      <td>40.728745</td>\n",
       "      <td>-74.032108</td>\n",
       "      <td>247578</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247579</th>\n",
       "      <td>455.0</td>\n",
       "      <td>2016-05-31 23:31:57</td>\n",
       "      <td>2016-05-31 23:39:32</td>\n",
       "      <td>3220</td>\n",
       "      <td>5 Corners Library</td>\n",
       "      <td>40.734961</td>\n",
       "      <td>-74.059503</td>\n",
       "      <td>3215</td>\n",
       "      <td>Central Ave</td>\n",
       "      <td>40.746730</td>\n",
       "      <td>-74.049251</td>\n",
       "      <td>247579</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247580</th>\n",
       "      <td>239.0</td>\n",
       "      <td>2016-05-31 23:47:38</td>\n",
       "      <td>2016-05-31 23:51:37</td>\n",
       "      <td>3185</td>\n",
       "      <td>City Hall</td>\n",
       "      <td>40.717732</td>\n",
       "      <td>-74.043845</td>\n",
       "      <td>3211</td>\n",
       "      <td>Newark Ave</td>\n",
       "      <td>40.721525</td>\n",
       "      <td>-74.046305</td>\n",
       "      <td>247580</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        trip_duration          start_time           stop_time  \\\n",
       "247576          250.0 2016-05-31 23:16:00 2016-05-31 23:20:10   \n",
       "247577          651.0 2016-05-31 23:18:32 2016-05-31 23:29:23   \n",
       "247578         2048.0 2016-05-31 23:25:28 2016-05-31 23:59:36   \n",
       "247579          455.0 2016-05-31 23:31:57 2016-05-31 23:39:32   \n",
       "247580          239.0 2016-05-31 23:47:38 2016-05-31 23:51:37   \n",
       "\n",
       "        start_station_id start_station_name  start_station_latitude  \\\n",
       "247576              3186      Grove St PATH               40.719586   \n",
       "247577              3209       Brunswick St               40.724176   \n",
       "247578              3199       Newport Pkwy               40.728745   \n",
       "247579              3220  5 Corners Library               40.734961   \n",
       "247580              3185          City Hall               40.717732   \n",
       "\n",
       "        start_station_longitude  end_station_id end_station_name  \\\n",
       "247576               -74.043117            3209     Brunswick St   \n",
       "247577               -74.050656            3211       Newark Ave   \n",
       "247578               -74.032108            3199     Newport Pkwy   \n",
       "247579               -74.059503            3215      Central Ave   \n",
       "247580               -74.043845            3211       Newark Ave   \n",
       "\n",
       "        end_station_latitude  end_station_longitude  bike_id   user_type  \\\n",
       "247576             40.724176             -74.050656   247576  Subscriber   \n",
       "247577             40.721525             -74.046305   247577  Subscriber   \n",
       "247578             40.728745             -74.032108   247578  Subscriber   \n",
       "247579             40.746730             -74.049251   247579  Subscriber   \n",
       "247580             40.721525             -74.046305   247580  Subscriber   \n",
       "\n",
       "        birth_year  gender  valid_trip_duration  \n",
       "247576      1976.0     1.0                 True  \n",
       "247577      1986.0     1.0                 True  \n",
       "247578      1975.0     1.0                 True  \n",
       "247579      1964.0     1.0                 True  \n",
       "247580      1993.0     1.0                 True  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bike.reset_index(drop=True, inplace=True)\n",
    "df_bike.bike_id = df_bike.index\n",
    "df_bike.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Loading Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = pd.read_csv('data/newark_airport_2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Inspecting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       STATION                                         NAME        DATE  \\\n",
      "0  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-01   \n",
      "1  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-02   \n",
      "2  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-03   \n",
      "3  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-04   \n",
      "4  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-05   \n",
      "\n",
      "    AWND  PGTM  PRCP  SNOW  SNWD  TAVG  TMAX  TMIN  TSUN  WDF2   WDF5  WSF2  \\\n",
      "0  12.75   NaN   0.0   0.0   0.0    41    43    34   NaN   270  280.0  25.9   \n",
      "1   9.40   NaN   0.0   0.0   0.0    36    42    30   NaN   260  260.0  21.0   \n",
      "2  10.29   NaN   0.0   0.0   0.0    37    47    28   NaN   270  250.0  23.9   \n",
      "3  17.22   NaN   0.0   0.0   0.0    32    35    14   NaN   330  330.0  25.9   \n",
      "4   9.84   NaN   0.0   0.0   0.0    19    31    10   NaN   360  350.0  25.1   \n",
      "\n",
      "   WSF5  \n",
      "0  35.1  \n",
      "1  25.1  \n",
      "2  30.0  \n",
      "3  33.1  \n",
      "4  31.1  \n"
     ]
    }
   ],
   "source": [
    "print(df_weather.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         STATION                                         NAME        DATE  \\\n",
      "361  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-12-27   \n",
      "362  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-12-28   \n",
      "363  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-12-29   \n",
      "364  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-12-30   \n",
      "365  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-12-31   \n",
      "\n",
      "      AWND  PGTM  PRCP  SNOW  SNWD  TAVG  TMAX  TMIN  TSUN  WDF2   WDF5  WSF2  \\\n",
      "361  13.65   NaN  0.01   0.0   0.0    53    62    40   NaN   270  270.0  29.1   \n",
      "362   8.28   NaN  0.00   0.0   0.0    41    43    31   NaN   330  330.0  19.9   \n",
      "363   8.05   NaN  0.36   0.0   0.0    38    45    31   NaN   170  150.0  18.1   \n",
      "364  14.99   NaN  0.00   0.0   0.0    37    42    32   NaN   270  270.0  25.9   \n",
      "365  12.30   NaN  0.00   0.0   0.0    35    44    29   NaN   200  220.0  21.9   \n",
      "\n",
      "     WSF5  \n",
      "361  38.0  \n",
      "362  25.1  \n",
      "363  25.1  \n",
      "364  33.1  \n",
      "365  28.0  \n"
     ]
    }
   ],
   "source": [
    "print(df_weather.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspection of the data there are a few observations to be made.\n",
    "- All `PGTM` values are missing.\n",
    "- All `PRCP` values are `0.0`.\n",
    "- All `SNOW` values are `0.0`.\n",
    "- All `SNWD` values are `0.0`.\n",
    "- All `TSUN` values are missing.\n",
    "- The values for the `NAME` column are identical.\n",
    "- The values for the `STATION` column are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Data\n",
    "- AWND: Average Daily Wind Speed (m/s)\n",
    "- PGTM: Peak Gust Time (HHMM)\n",
    "- PRCP: Precipitation (mm)\n",
    "- SNOW: Snowfall (mm)\n",
    "- SNWD: Snow Depth (mm)\n",
    "- TAVG: Temperature Average\n",
    "- TMIN: Minimum Temp\n",
    "- TSUN: Daily Total Sunshine (minutes)\n",
    "- WDF2: Direction of Fastest 2 Minute Wind (degrees)\n",
    "- WDF5: Direction of Fastest 5 Minute Wind (degrees)\n",
    "- WSF2: Fastest 2 Min Wind Speed (m/s)\n",
    "- WSF5: Fastest 5 Min Wind Speed (m/s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is significant Snowfall, we would expect Snow Depth. This is something we can investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.columns = [x.lower() for x in df_weather.columns]\n",
    "df_weather = df_weather.rename(columns={'snow': 'snow_amt', 'snwd': 'snow_depth', 'awnd': 'avg_wind_speed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['station', 'name', 'date', 'avg_wind_speed', 'pgtm', 'prcp', 'snow_amt',\n",
      "       'snow_depth', 'tavg', 'tmax', 'tmin', 'tsun', 'wdf2', 'wdf5', 'wsf2',\n",
      "       'wsf5'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_weather.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Inspecting `name` and `station` Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data showed all the values for the `name` and `station` column as identical we want to inspect the data to see if this is a consistent pattern throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station\n",
       "USW00014734    366\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather.station.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US    366\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather.name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above demonstrates that the same value is present for the `name` and `station` columns throughout the dataset. Therefore we can drop the `name` and `station` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = df_weather.drop(['station', 'name'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Missing `tsun` Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect how much of the `tsun` data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather[df_weather.tsun.isnull() != False].shape[0] / df_weather.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the `tsun` data is missing. Keeping this data will lead to database inefficiencies, hence we should remove this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = df_weather.drop('tsun', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'avg_wind_speed', 'pgtm', 'prcp', 'snow_amt', 'snow_depth',\n",
       "       'tavg', 'tmax', 'tmin', 'wdf2', 'wdf5', 'wsf2', 'wsf5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Missing `pgtm` Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect how much of the `pgtm` data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather[df_weather.pgtm.isnull() != False].shape[0] / df_weather.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the `pgtm` data is missing. Keeping this data will lead to database inefficiences, hence we should remove this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = df_weather.drop('pgtm', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'avg_wind_speed', 'prcp', 'snow_amt', 'snow_depth', 'tavg',\n",
       "       'tmax', 'tmin', 'wdf2', 'wdf5', 'wsf2', 'wsf5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Columns `wdf2` `wdf5` `wsf2` `wsf5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns are not really relevant as we do not know what time of day they occurred at and so it will be hard to link it to our other data. It is best we drop this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = df_weather.drop(['wdf2', 'wdf5', 'wsf2', 'wsf5'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Checking Column Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date              datetime64[s]\n",
      "avg_wind_speed          float64\n",
      "prcp                    float64\n",
      "snow_amt                float64\n",
      "snow_depth              float64\n",
      "tavg                      int64\n",
      "tmax                      int64\n",
      "tmin                      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_weather.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.date = df_weather.date.astype('datetime64[s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date              datetime64[s]\n",
      "avg_wind_speed          float64\n",
      "prcp                    float64\n",
      "snow_amt                float64\n",
      "snow_depth              float64\n",
      "tavg                      int64\n",
      "tmax                      int64\n",
      "tmin                      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_weather.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Inspecting `snow` and `snwd` Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_snow = df_weather[(df_weather.snow_amt != 0) | (df_weather.snow_depth != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather_snow.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>avg_wind_speed</th>\n",
       "      <th>prcp</th>\n",
       "      <th>snow_amt</th>\n",
       "      <th>snow_depth</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>9.40</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "      <td>42</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>17.22</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>10.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2016-01-23</td>\n",
       "      <td>22.82</td>\n",
       "      <td>1.81</td>\n",
       "      <td>24.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2016-01-24</td>\n",
       "      <td>9.40</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.1</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  avg_wind_speed  prcp  snow_amt  snow_depth  tavg  tmax  tmin\n",
       "16 2016-01-17            9.40  0.07       0.7         0.0    38    42    29\n",
       "17 2016-01-18           17.22  0.03       0.5         1.2    27    30    18\n",
       "21 2016-01-22           10.29  0.03       0.3         0.0    26    30    20\n",
       "22 2016-01-23           22.82  1.81      24.0         7.1    26    27    23\n",
       "23 2016-01-24            9.40  0.01       0.2        20.1    26    36    17"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather_snow.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that on 2016-01-23 there was 7.1mm of snow depth however the next day there was 20.1mm of snow depth despite there only being 0.2mm of snowfall the next day. This may be some error in reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Snow and Rain Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add snow and rain columns which allow for convenient analysis on whether it snowed or rained on that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather['snow'] = df_weather.snow_amt.apply(lambda x: 1 if x > 0 else 0).astype('bool')\n",
    "df_weather['rain'] = df_weather.prcp.apply(lambda x: 1 if x > 0 else 0).astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  avg_wind_speed  prcp  snow_amt  snow_depth  tavg  tmax  tmin  \\\n",
      "0 2016-01-01           12.75   0.0       0.0         0.0    41    43    34   \n",
      "1 2016-01-02            9.40   0.0       0.0         0.0    36    42    30   \n",
      "2 2016-01-03           10.29   0.0       0.0         0.0    37    47    28   \n",
      "3 2016-01-04           17.22   0.0       0.0         0.0    32    35    14   \n",
      "4 2016-01-05            9.84   0.0       0.0         0.0    19    31    10   \n",
      "\n",
      "    snow   rain  \n",
      "0  False  False  \n",
      "1  False  False  \n",
      "2  False  False  \n",
      "3  False  False  \n",
      "4  False  False  \n"
     ]
    }
   ],
   "source": [
    "print(df_weather.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_wind_speed</th>\n",
       "      <th>prcp</th>\n",
       "      <th>snow_amt</th>\n",
       "      <th>snow_depth</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>366.000000</td>\n",
       "      <td>366.000000</td>\n",
       "      <td>366.000000</td>\n",
       "      <td>366.000000</td>\n",
       "      <td>366.000000</td>\n",
       "      <td>366.000000</td>\n",
       "      <td>366.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.429973</td>\n",
       "      <td>0.104945</td>\n",
       "      <td>0.098087</td>\n",
       "      <td>0.342623</td>\n",
       "      <td>57.196721</td>\n",
       "      <td>65.991803</td>\n",
       "      <td>48.459016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.748174</td>\n",
       "      <td>0.307496</td>\n",
       "      <td>1.276498</td>\n",
       "      <td>2.078510</td>\n",
       "      <td>17.466981</td>\n",
       "      <td>18.606301</td>\n",
       "      <td>17.135790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.765000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>51.250000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.720000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.410000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.820000</td>\n",
       "      <td>2.790000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>20.100000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       avg_wind_speed        prcp    snow_amt  snow_depth        tavg  \\\n",
       "count      366.000000  366.000000  366.000000  366.000000  366.000000   \n",
       "mean         9.429973    0.104945    0.098087    0.342623   57.196721   \n",
       "std          3.748174    0.307496    1.276498    2.078510   17.466981   \n",
       "min          2.460000    0.000000    0.000000    0.000000    8.000000   \n",
       "25%          6.765000    0.000000    0.000000    0.000000   43.000000   \n",
       "50%          8.720000    0.000000    0.000000    0.000000   56.000000   \n",
       "75%         11.410000    0.030000    0.000000    0.000000   74.000000   \n",
       "max         22.820000    2.790000   24.000000   20.100000   89.000000   \n",
       "\n",
       "             tmax        tmin  \n",
       "count  366.000000  366.000000  \n",
       "mean    65.991803   48.459016  \n",
       "std     18.606301   17.135790  \n",
       "min     18.000000    0.000000  \n",
       "25%     51.250000   35.000000  \n",
       "50%     66.000000   47.000000  \n",
       "75%     83.000000   64.000000  \n",
       "max     99.000000   80.000000  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather[['avg_wind_speed', 'prcp', 'snow_amt', 'snow_depth', 'tavg', 'tmax', 'tmin']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Creating Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Dimension Table\n",
    "\n",
    "As our data frequently uses the date, it makes sense to get all the data and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date = pd.DataFrame({\"date\": pd.date_range('2016-01-01', '2016-12-31')})\n",
    "df_date[\"day\"] = df_date.date.dt.day_name()\n",
    "df_date[\"week\"] = df_date.date.dt.isocalendar().week\n",
    "df_date[\"quarter\"] = df_date.date.dt.quarter\n",
    "df_date[\"year\"] = df_date.date.dt.year\n",
    "df_date[\"weekend\"] = df_date.day.apply(lambda x: 1 if x == 'Saturday' or x == 'Sunday' else 0).astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date['date_key'] = df_date.date.apply(lambda x: int(x.strftime('%Y%m%d').strip('-')))\n",
    "df_bike['date_key'] = df_bike.start_time.apply(lambda x: int(x.strftime('%Y%m%d').strip('-')))\n",
    "df_weather['date_key'] = df_weather.date.apply(lambda x: int(x.strftime('%Y%m%d').strip('-')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date        datetime64[ns]\n",
      "day                 object\n",
      "week                UInt32\n",
      "quarter              int32\n",
      "year                 int32\n",
      "weekend               bool\n",
      "date_key             int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_date.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rides table will use the bike ID as it's Index, hence no index will be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   trip_duration          start_time           stop_time  start_station_id  \\\n",
      "0          361.0 2016-02-01 00:31:18 2016-02-01 00:37:19              3202   \n",
      "1          297.0 2016-02-01 01:55:05 2016-02-01 02:00:02              3195   \n",
      "2         1155.0 2016-02-01 02:40:05 2016-02-01 02:59:20              3183   \n",
      "3         1770.0 2016-02-01 05:11:28 2016-02-01 05:40:58              3214   \n",
      "4          935.0 2016-02-01 05:48:24 2016-02-01 06:03:59              3203   \n",
      "\n",
      "  start_station_name  start_station_latitude  start_station_longitude  \\\n",
      "0       Newport PATH               40.727224               -74.033759   \n",
      "1            Sip Ave               40.730743               -74.063784   \n",
      "2     Exchange Place               40.716247               -74.033459   \n",
      "3   Essex Light Rail               40.712774               -74.036486   \n",
      "4      Hamilton Park               40.727596               -74.044247   \n",
      "\n",
      "   end_station_id  end_station_name  end_station_latitude  \\\n",
      "0            3203     Hamilton Park             40.727596   \n",
      "1            3194   McGinley Square             40.725340   \n",
      "2            3210    Pershing Field             40.742677   \n",
      "3            3203     Hamilton Park             40.727596   \n",
      "4            3214  Essex Light Rail             40.712774   \n",
      "\n",
      "   end_station_longitude  bike_id  valid_trip_duration  date_key  \n",
      "0             -74.044247        0                 True  20160201  \n",
      "1             -74.067622        1                 True  20160201  \n",
      "2             -74.051789        2                 True  20160201  \n",
      "3             -74.044247        3                 True  20160201  \n",
      "4             -74.036486        4                 True  20160201  \n"
     ]
    }
   ],
   "source": [
    "df_users = df_bike[['user_type', 'gender', 'birth_year']].drop_duplicates().reset_index(drop=True)\n",
    "df_users['id'] = df_users.index\n",
    "rides = df_bike.merge(df_users, on=['user_type', 'gender', 'birth_year']).sort_values(by='bike_id')\n",
    "rides = rides.drop(['user_type', 'gender', 'birth_year', 'id'], axis=1)\n",
    "print(rides.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['trip_duration', 'start_time', 'stop_time', 'start_station_id',\n",
       "       'start_station_name', 'start_station_latitude',\n",
       "       'start_station_longitude', 'end_station_id', 'end_station_name',\n",
       "       'end_station_latitude', 'end_station_longitude', 'bike_id',\n",
       "       'valid_trip_duration', 'date_key'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id              name   latitude  longitude\n",
      "0  3202      Newport PATH  40.727224 -74.033759\n",
      "1  3195           Sip Ave  40.730743 -74.063784\n",
      "2  3183    Exchange Place  40.716247 -74.033459\n",
      "3  3214  Essex Light Rail  40.712774 -74.036486\n",
      "4  3203     Hamilton Park  40.727596 -74.044247\n"
     ]
    }
   ],
   "source": [
    "start_station = rides[['start_station_id', 'start_station_name', 'start_station_latitude', 'start_station_longitude']]\n",
    "end_station = rides[['end_station_id', 'end_station_name', 'end_station_latitude', 'end_station_longitude']]\n",
    "\n",
    "start_station = start_station.rename(columns={'start_station_id': 'id', 'start_station_name': 'name', 'start_station_latitude': 'latitude', 'start_station_longitude': 'longitude'})\n",
    "end_station = end_station.rename(columns={'end_station_id': 'id', 'end_station_name': 'name', 'end_station_latitude': 'latitude', 'end_station_longitude': 'longitude'})\n",
    "\n",
    "station = pd.concat([start_station, end_station]).drop_duplicates()\n",
    "print(station.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rides table already contains the `start_station_id` and the `end_station_id`, hence no merge is required. Any column relating to a station that is not the index can be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides = rides.drop(['start_station_name', 'start_station_latitude', 'start_station_longitude', 'end_station_name', 'end_station_latitude', 'end_station_longitude'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   trip_duration          start_time           stop_time  start_station_id  \\\n",
      "0          361.0 2016-02-01 00:31:18 2016-02-01 00:37:19              3202   \n",
      "1          297.0 2016-02-01 01:55:05 2016-02-01 02:00:02              3195   \n",
      "2         1155.0 2016-02-01 02:40:05 2016-02-01 02:59:20              3183   \n",
      "3         1770.0 2016-02-01 05:11:28 2016-02-01 05:40:58              3214   \n",
      "4          935.0 2016-02-01 05:48:24 2016-02-01 06:03:59              3203   \n",
      "\n",
      "   end_station_id  bike_id  valid_trip_duration  date_key  \n",
      "0            3203        0                 True  20160201  \n",
      "1            3194        1                 True  20160201  \n",
      "2            3210        2                 True  20160201  \n",
      "3            3203        3                 True  20160201  \n",
      "4            3214        4                 True  20160201  \n"
     ]
    }
   ],
   "source": [
    "print(rides.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trip_duration                float64\n",
       "start_time             datetime64[s]\n",
       "stop_time              datetime64[s]\n",
       "start_station_id               int64\n",
       "end_station_id                 int64\n",
       "bike_id                        int64\n",
       "valid_trip_duration             bool\n",
       "date_key                       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather['id'] = df_weather.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  avg_wind_speed  prcp  snow_amt  snow_depth  tavg  tmax  tmin  \\\n",
      "0 2016-01-01           12.75   0.0       0.0         0.0    41    43    34   \n",
      "1 2016-01-02            9.40   0.0       0.0         0.0    36    42    30   \n",
      "2 2016-01-03           10.29   0.0       0.0         0.0    37    47    28   \n",
      "3 2016-01-04           17.22   0.0       0.0         0.0    32    35    14   \n",
      "4 2016-01-05            9.84   0.0       0.0         0.0    19    31    10   \n",
      "\n",
      "    snow   rain  date_key  id  \n",
      "0  False  False  20160101   0  \n",
      "1  False  False  20160102   1  \n",
      "2  False  False  20160103   2  \n",
      "3  False  False  20160104   3  \n",
      "4  False  False  20160105   4  \n"
     ]
    }
   ],
   "source": [
    "print(df_weather.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date              datetime64[s]\n",
      "avg_wind_speed          float64\n",
      "prcp                    float64\n",
      "snow_amt                float64\n",
      "snow_depth              float64\n",
      "tavg                      int64\n",
      "tmax                      int64\n",
      "tmin                      int64\n",
      "snow                       bool\n",
      "rain                       bool\n",
      "date_key                  int64\n",
      "id                        int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_weather.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weather table can remain as is as there are no logical groupings to split out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Connect to a PostGres Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sal\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id              name   latitude  longitude\n",
      "0  3202      Newport PATH  40.727224 -74.033759\n",
      "1  3195           Sip Ave  40.730743 -74.063784\n",
      "2  3183    Exchange Place  40.716247 -74.033459\n",
      "3  3214  Essex Light Rail  40.712774 -74.036486\n",
      "4  3203     Hamilton Park  40.727596 -74.044247\n"
     ]
    }
   ],
   "source": [
    "print(station.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sal.create_engine('postgresql://arbergllogjani:@127.0.0.1:10000/codecademydataengineering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acknowledge Data Uploaded\n"
     ]
    }
   ],
   "source": [
    "df_date.to_sql('date_dimension', connection, if_exists='append', index=False, chunksize=10000)\n",
    "station.to_sql('station', connection, if_exists='append', index=False, chunksize=10000)\n",
    "df_users.to_sql('users', connection, if_exists='append', index=False, chunksize=10000)\n",
    "df_weather.to_sql('weather', connection, if_exists='append', index=False, chunksize=10000)\n",
    "rides.to_sql('rides', connection, if_exists='append', index=False, chunksize=10000)\n",
    "print('Acknowledge Data Uploaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
